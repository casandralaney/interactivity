---
title: "iterations_and_listcols"
author: "Casandra Laney"
date: "2025-10-28"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(rvest)
```

```{r}
set.seed(1)
```

```{r}
list_norms = 
  list(
    a = rnorm(20, 3, 1),
    b = rnorm(20, 0, 5),
    c = rnorm(20, 10, .2),
    d = rnorm(20, -3, 1)
  )

is.list(list_norms)
```

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)

  tibble(
    mean = mean_x, 
    sd = sd_x
  )
}
```

```{r}
mean_and_sd(list_norms[[1]])
mean_and_sd(list_norms[[2]])
mean_and_sd(list_norms[[3]])
mean_and_sd(list_norms[[4]])
```

Replicate this process using a for loop

```{r}
output = vector("list", length = 4)

for (i in 1:4) {
  
  output[[i]] = mean_and_sd(list_norms[[i]])
}

output
```


Same thing as for loop, just one line of code
```{r}
output = map(list_norms, mean_and_sd)

output
```


MAP variants
```{r}
map_dfr(list_norms, mean_and_sd, .id = "sample")

map_dbl(list_norms, median)
```

## List Columns

Try to put my list into a df

```{r}

listcol_df =
  tibble(
    name = c("a", "b", "c", "d"),
    sample = list_norms
  )

rnorm
```

did this really work?

```{r}
pull(listcol_df, name)
pull(listcol_df, sample)

```

Can I apply 'mean_and_sd'??

```{r}
mean_and_sd(pull(listcol_df, sample)[[1]])
mean_and_sd(pull(listcol_df, sample)[[2]])
mean_and_sd(pull(listcol_df, sample)[[3]])
mean_and_sd(pull(listcol_df, sample)[[4]])

```


iterate using 'map'

```{r}
map(pull(listcol_df, sample), mean_and_sd)
```

adding a column...

```{r}
listcol_df = 
  listcol_df |>
  mutate(
    summary = map(sample, mean_and_sd)
  )
```



```{r}
listcol_df |>
  select(-sample) |>
  unnest(summary)
```

example 

```{r}
nsduh_table <- function(html, table_num) {
  
  table = 
    html |> 
    html_table() |> 
    nth(table_num) |>
    slice(-1) |> 
    select(-contains("P Value")) |>
    pivot_longer(
      -State,
      names_to = "age_year", 
      values_to = "percent") |>
    separate(age_year, into = c("age", "year"), sep = "\\(") |>
    mutate(
      year = str_replace(year, "\\)", ""),
      percent = str_replace(percent, "[a-c]$", ""),
      percent = as.numeric(percent)) |>
    filter(!(State %in% c("Total U.S.", "Northeast", "Midwest", "South", "West")))
  
  table
}
```


using for loop
```{r}

nsduh_url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"

nsduh_html = read_html(nsduh_url)

output = vector("list", 3)

for (i in c(1, 4, 5)) {
  output[[i]] = nsduh_table(nsduh_html, i)
}

nsduh_results = bind_rows(output)
```


using map
```{r}
nsduh_results = 
  map(c(1, 4, 5), nsduh_table, html = nsduh_html) |> 
  bind_rows()
```



```{r}
nsduh_results = 
  tibble(
    name = c("marj", "cocaine", "heroine"),
    number = c(1, 4, 5)) |> 
  mutate(table = map(number, nsduh_table, html = nsduh_html)) |> 
  unnest(cols = "table")
```

## Look at weather data

```{r}

library(p8105.datasets)
data("weather_df")

```


```{r}
weather_df |>
  filter(name == "CentralPark_NY") |>
  ggplot(aes(x = tmin, y = tmax)) +
  geom_point() +
  geom_smooth(method = "lm")
```

let's do a regression

```{r}
weather_df |>
  filter(name == "CentralPark_NY") |>
  lm(tmax ~ tmin, data = _)

weather_df |>
  filter(name == "Molokai_HI") |>
  lm(tmax ~ tmin, data = _)

weather_df |>
  filter(name == "Waterhole_WA") |>
  lm(tmax ~ tmin, data = _)
```

Let's iterate differently
```{r}
weather_nest =
  weather_df |>
  nest(data = date:tmin)
```

```{r}
lm(tmax ~ tmin, data = pull(weather_nest, data)[[1]])
lm(tmax ~ tmin, data = pull(weather_nest, data)[[2]])
lm(tmax ~ tmin, data = pull(weather_nest, data)[[3]])

```

do this using map

```{r}
weather_lm = function(df) {
  
  lm(tmax ~ tmin, data = df)
  
}


map(pull(weather_nest, data), weather_lm)
```


```{r}
weather_nest |>
  mutate(
    lm_fits = map(data, weather_lm)
    )
  
```

